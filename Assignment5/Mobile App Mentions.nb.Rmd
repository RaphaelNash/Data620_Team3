---
title: "R Notebook"
output: html_notebook
---

```{r, echo=FALSE, warning=FALSE, message=FALSE}
#Required libraries
library(stringr)
library(tm)
library(SnowballC)
library(RCurl)
library(NLP)
library(RTextTools)

#Required Libraries for Charts and Graphs
library(wordcloud)
library(ggplot2)
library(cluster)
library(dplyr)
```

```{r}
#Corpus Import & Syntax
#fileencoding needs to be set to "latin1"
MentionsURL <-"https://raw.githubusercontent.com/Misterresearch/CUNY-Projects/master/App%20Mentions.csv"

MentionsTable <- read.csv(file = MentionsURL, header = TRUE, sep = ",", strip.white = TRUE, na.strings = "", fill = TRUE, fileEncoding = "latin1")
MentionsTable$description = as.character(MentionsTable$description)

#Create data frame
mentiondesc <- data.frame(MentionsTable$description)

#makes each row in data frame a document, required for subsequent statistical analysis.
mentiondesc <- Corpus(DataframeSource(mentiondesc))

#Corpus Loading, filtering and stemming code. See "Basic Text Mining" source in end notes.
mentiondesc <- tm_map(mentiondesc, removePunctuation)
#for(j in seq(mentiondesc))   
#{   
  #mentiondesc[[j]] <- gsub("/", " ", mentiondesc[[j]])   
  #mentiondesc[[j]] <- gsub("@", " ", mentiondesc[[j]])   
  #mentiondesc[[j]] <- gsub("\\|", " ", mentiondesc[[j]])   
#} 
mentiondesc <- tm_map(mentiondesc, removeNumbers)
mentiondesc <- tm_map(mentiondesc, tolower)
mentiondesc <- tm_map(mentiondesc, removeWords, stopwords("english"))
mentiondesc <- tm_map(mentiondesc, removeWords, c("none","the", "and", "or" , "http\\w*"))
mentiondesc <- tm_map(mentiondesc, stemDocument)
mentiondesc <- tm_map(mentiondesc, stripWhitespace)
mentiondesc <- tm_map(mentiondesc, PlainTextDocument)

#Single Term Matrices
mdtm <- DocumentTermMatrix(mentiondesc)
mtdm <- TermDocumentMatrix(mentiondesc)
mdtm
mtdm

#Sparcity settng adjustments
mentionstdm2 <- removeSparseTerms(mtdm, .97)
mentionsdtm2 <- removeSparseTerms(mdtm, .97)
mentionsfreq <- rowSums(as.matrix(mtdm))
#findFreqTerms(mentionstdm2, lowfreq = 1)
```


```{r}
#Single Term Frequency Charts
tf <- data.frame(term = names(mentionsfreq), instances=mentionsfreq)
subset(tf, mentionsfreq>100) %>%
  ggplot(aes(term,instances)) +
  geom_bar(stat="identity", fill="darkblue", colour="darkred") +
  theme(axis.text.x=element_text(angle = 45, hjust = 1))

#Single Term Word Clouds
wordcloud(names(mentionsfreq), mentionsfreq, min.freq = 100, scale=c(5, .1), colors=brewer.pal(6, "Dark2"))

#Single Term Correlation Analysis
findAssocs(mtdm, c("Twitter", "Amazon", "chromecast"), corlimit = .2)

#Single Term Cluster Analysis, requires Document-Text Matrix

dendro <- dist(t(mentionsdtm2), method="euclidean")
cluster <- hclust(d=dendro, method="ward.D")
plot(cluster, hang=-1)
rect.hclust(cluster, k=5, border="orange")

#Bigram Corpus, see source for "Bigram Text-Document Matrices" in endnotes
BigramTokenizer <-
     function(x)
        unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
mtdm2 <- TermDocumentMatrix(mentiondesc, control = list(tokenize = BigramTokenizer))
mdtm2 <- DocumentTermMatrix(mentiondesc, control = list(tokenize = BigramTokenizer))
mentionsfreq2 <- rowSums(as.matrix(mtdm2))


#Bigram Frequency Chart
tf <- data.frame(term = names(mentionsfreq2), instances=mentionsfreq2)
subset(tf, mentionsfreq2>50) %>%
  ggplot(aes(term,instances)) +
  geom_bar(stat="identity", fill="darkblue", colour="darkred") +
  theme(axis.text.x=element_text(angle = 45, hjust = 1))
#Bigram Word Cloud
wordcloud(names(mentionsfreq2), mentionsfreq2, min.freq = 25, scale=c(5, .1), colors=brewer.pal(6, "Dark2"))

#Bigram Dendrogram
mdtm2a <- removeSparseTerms(mdtm2, .993)
dendro2 <- dist(t(mdtm2a), method="euclidean")
cluster <- hclust(d=dendro2, method="ward.D")
plot(cluster, hang=-1)
rect.hclust(cluster, k=2, border="orange")
```


Mentions Classification & Analysis

```{r}
#pos and neg sentiment sourced from Hu and Liu
#header notes removed from source file, adjust file path
pos_words = read.table("/Users/digitalmarketer1977/Desktop/positive-words.txt", header = F, stringsAsFactors = F)[, 1]
neg_words = read.table("/Users/digitalmarketer1977/Desktop/negative-words.txt", header = F, stringsAsFactors = F)[, 1]

#append to data frame
MentionsTable$neg = sapply(mentiondesc, tm_term_score, neg_words)
MentionsTable$pos = sapply(mentiondesc, tm_term_score, pos_words)

#Raw Sum of Neg vs. Pos Comments
sumpos <- sum(MentionsTable$pos)
sumneg <- sum(MentionsTable$neg)
sumcomments <- barplot(c(sumneg,sumpos),col = c("red","darkgreen"), ylim = c(0,3500))
text(x = sumcomments, y = c(sumneg,sumpos),labels = c(sumneg,sumpos), pos = 3 , cex = .8, col = "black")
#Sentiment Classification & Analysis
MentionsTable$sentiment = MentionsTable$pos/MentionsTable$neg
MentionsTable$sentiment[MentionsTable$sentiment > 1.50] <- "Positive"
MentionsTable$sentiment[(MentionsTable$sentiment <= 1.50) & (MentionsTable$sentiment >= .50)] <- "Neutral"
MentionsTable$sentiment[MentionsTable$sentiment == "NaN"] <- "Neutral"
MentionsTable$sentiment[MentionsTable$sentiment < .50] <- "Negative"
sentiment <- table(MentionsTable$sentiment)
myValues <- barplot(sentiment, main="Custom Sentiment", xlab = "Sentiment Category", col=c("red","darkblue", "darkgreen"), border = "black", ylim = c(0,4000))
text(x = myValues, y = sentiment, labels = sentiment, pos = 3, cex = .8, col = "black")
tone <- table(MentionsTable$tone)
myValuesB <- barplot(tone, main="Mentions Tone", xlab = "Tone Category", col=c("red","darkblue","darkgreen"), border = "black", ylim = c(0,4000))
text(x = myValuesB, y = tone, labels = tone, pos = 3, cex = .8, col = "black")
```


Code Source: <a href=https://rstudio-pubs-static.s3.amazonaws.com/31867_8236987cf0a8444e962ccd2aec46d9c3.html>Basic Text Mining in R</a>

Code Source: <a href=http://tm.r-forge.r-project.org/faq.html#Bigrams>Bigram Text-Document Matrices</a>

Reference: Automated Data Collection with R, Wiley (2015)

Code Source: <a href=https://rstudio-pubs-static.s3.amazonaws.com/132792_864e3813b0ec47cb95c7e1e2e2ad83e7.html>Predictive Modeling</a>

Data Source: Minqing Hu and Bing Liu. "Mining and Summarizing Customer Reviews." 